<?xml version="1.0"?>
<pdf>
  <page width="612" height="792" number="1">
    <region x="97.63" y="737.85" width="416.74" height="9.08" line_height="9.08" font="XPTMOY+TimesNewRomanPSMT">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">The 6th IEEE International Conference on Wireless, Mobile, and Ubiquitous Technologies in Education</line>
    </region>
    <region x="78.96" y="677.85" width="457.51" height="31.62" line_height="15.54" font="OCAONN+TimesNewRoman,Bold">
      <line x_offset="0.0" y_offset="16.08" spacing="0.0">Development of Web-based Japanese Mimicry and Onomatopoeia Learning </line>
      <line x_offset="111.96" y_offset="0.0" spacing="0.54">Assistant System with Sensor Network </line>
    </region>
    <region x="147.0" y="581.89" width="320.72" height="60.23" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="48.01" spacing="0.0">Bin Hou, Hiroaki Ogata, Masayuki Miyata, Mengmeng Li, Yoneo Yano </line>
      <line x_offset="52.44" y_offset="34.56" spacing="2.42">Dept. of Information Science and Intelligent Systems </line>
      <line x_offset="99.72" y_offset="23.04" spacing="0.49">The University of Tokushima </line>
      <line x_offset="122.64" y_offset="11.52" spacing="0.49">Tokushima, Japan </line>
      <line x_offset="90.0" y_offset="0.0" spacing="0.49">houbin2008@is.tokushima-u.ac.jp </line>
    </region>
    <region x="54.0" y="444.69" width="245.61" height="113.53" line_height="9.96" font="OCAOOB+TimesNewRoman,BoldItalic">
      <line x_offset="0.0" y_offset="103.56" spacing="0.0">Abstract- In this paper, we propose a web-based Japanese </line>
      <line x_offset="0.0" y_offset="93.24" spacing="0.36">mimicry </line>
      <line x_offset="42.05" y_offset="93.24" spacing="-9.96">and </line>
      <line x_offset="67.66" y_offset="93.24" spacing="-9.96">onomatopoeia </line>
      <line x_offset="131.69" y_offset="93.24" spacing="-9.96">learning </line>
      <line x_offset="174.79" y_offset="93.24" spacing="-9.96">assistant </line>
      <line x_offset="218.42" y_offset="93.24" spacing="-9.96">system </line>
      <line x_offset="0.0" y_offset="82.8" spacing="0.48">(JAMIOLAS). In our previous studies, we have proposed </line>
      <line x_offset="0.0" y_offset="72.48" spacing="0.36">context-aware language learning assistant systems that used </line>
      <line x_offset="0.0" y_offset="62.16" spacing="0.36">wearable sensor and sensor network respectively, and attended </line>
      <line x_offset="0.0" y_offset="51.84" spacing="0.36">good results. In order to use this learning model in broader area </line>
      <line x_offset="0.0" y_offset="41.4" spacing="0.48">and more general scene, we are trying to realize the system on </line>
      <line x_offset="0.0" y_offset="31.08" spacing="0.36">website, and using on-line information as sensor data from </line>
      <line x_offset="0.0" y_offset="20.76" spacing="0.36">global sensor network. Besides, in order to support more words, </line>
      <line x_offset="0.0" y_offset="10.44" spacing="0.36">we are also using on-line multimedia such as video or picture to </line>
      <line x_offset="0.0" y_offset="0.0" spacing="0.48">create the context for learning. </line>
    </region>
    <region x="262.42" y="424.41" width="36.69" height="9.96" line_height="9.96" font="OCAOOB+TimesNewRoman,BoldItalic">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">language </line>
    </region>
    <region x="54.0" y="414.09" width="203.74" height="20.28" line_height="9.96" font="OCAOOB+TimesNewRoman,BoldItalic">
      <line x_offset="14.4" y_offset="10.32" spacing="0.0">Keywords-mimicry; </line>
      <line x_offset="99.92" y_offset="10.32" spacing="-9.96">onomatopoeia; </line>
      <line x_offset="168.51" y_offset="10.32" spacing="-9.96">sensor; </line>
      <line x_offset="0.0" y_offset="0.0" spacing="0.36">learning; context-aware learning; ubiquitous learning </line>
    </region>
    <region x="54.0" y="75.73" width="246.23" height="324.25" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="83.4" y_offset="313.08" spacing="0.0">I. </line>
      <line x_offset="104.28" y_offset="313.08" spacing="-11.03"> NTRODUCTIONI</line>
      <line x_offset="14.4" y_offset="298.08" spacing="3.97">Context-aware computing [1] will help in the </line>
      <line x_offset="0.0" y_offset="287.04" spacing="0.01">organization and mediation of social interactions wherever </line>
      <line x_offset="0.0" y_offset="276.12" spacing="-0.11">and whenever these contexts might occur [2]. Context-aware </line>
      <line x_offset="0.0" y_offset="265.2" spacing="-0.11">computing makes it possible to learning foreign language </line>
      <line x_offset="0.0" y_offset="254.28" spacing="-0.11">words related to people's feeling more comfortably. </line>
      <line x_offset="14.4" y_offset="243.36" spacing="-0.11">Computer Supported Ubiquitous Learning (CSUL) has </line>
      <line x_offset="0.0" y_offset="232.44" spacing="-0.11">integrated high mobility with embedded computing </line>
      <line x_offset="0.0" y_offset="221.52" spacing="-0.11">environments [3,4]. We are focusing on applying CSUL to </line>
      <line x_offset="0.0" y_offset="210.6" spacing="-0.11">language learning and are investigating computer supported </line>
      <line x_offset="0.0" y_offset="199.68" spacing="-0.11">ubiquitous learning [4]. We proposed context-aware </line>
      <line x_offset="0.0" y_offset="188.76" spacing="-0.11">language learning assistant system called JAMIOLAS [5-7] </line>
      <line x_offset="0.0" y_offset="177.84" spacing="-0.11">for learning Japanese mimicry and onomatopoeia (MIO) </line>
      <line x_offset="0.0" y_offset="166.92" spacing="-0.11">words. The previous two studies used wearable sensors and </line>
      <line x_offset="0.0" y_offset="156.0" spacing="-0.11">sensor network respectively to detect the context </line>
      <line x_offset="0.0" y_offset="145.08" spacing="-0.11">automatically and achieved certain effect. However, it still </line>
      <line x_offset="0.0" y_offset="134.16" spacing="-0.11">cannot meet learner needs. Therefore, in this paper we </line>
      <line x_offset="0.0" y_offset="123.24" spacing="-0.11">propose an improved system named JAMIOLAS 3.0 that can </line>
      <line x_offset="0.0" y_offset="112.32" spacing="-0.11">support learning MIO by using sensor data. </line>
      <line x_offset="102.6" y_offset="93.42" spacing="10.0">IMICRY ND NOMATOPOEIA</line>
      <line x_offset="93.6" y_offset="93.0" spacing="-10.61">M A O</line>
      <line x_offset="29.16" y_offset="93.0" spacing="-11.02">II.</line>
      <line x_offset="51.72" y_offset="93.0" spacing="-11.03">JAPANESE </line>
      <line x_offset="14.4" y_offset="77.88" spacing="4.09">Mimicry words are imitating situations and body </line>
      <line x_offset="0.0" y_offset="66.96" spacing="-0.11">movements while onomatopoeia shows sounds of something </line>
      <line x_offset="0.0" y_offset="56.04" spacing="-0.11">[7]. Japanese is very rich in it. It is very important but very </line>
      <line x_offset="14.4" y_offset="32.71" spacing="10.35">&#x2022;</line>
      <line x_offset="14.4" y_offset="10.15" spacing="9.58">&#x2022;</line>
      <line x_offset="0.0" y_offset="45.12" spacing="-45.99">difficult to learn because of following aspects: </line>
      <line x_offset="32.4" y_offset="33.48" spacing="0.61">Explanation: Nearly all of MIO words are just </line>
      <line x_offset="32.4" y_offset="22.56" spacing="-0.11">feeling of Japanese. </line>
      <line x_offset="32.4" y_offset="10.92" spacing="0.61">Translation: Difficult to find the word that has the </line>
      <line x_offset="32.4" y_offset="0.0" spacing="-0.11">exactly same meaning in other language. </line>
    </region>
    <region x="315.0" y="73.21" width="246.31" height="486.09" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="32.4" y_offset="473.88" spacing="1.19">Writing: Most of MIO words are written in hiragana </line>
      <line x_offset="14.4" y_offset="473.11" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="462.96" spacing="-0.87">or katakana (Japanese syllabify), not in kanji. It is </line>
      <line x_offset="32.4" y_offset="452.04" spacing="-0.11">easy to pronounce but difficult to understand. </line>
      <line x_offset="32.4" y_offset="440.4" spacing="0.61">Hearing and Saying: The pronunciation of MIO </line>
      <line x_offset="14.4" y_offset="439.63" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="429.48" spacing="-0.87">usually has twice repetitions. It may cause the </line>
      <line x_offset="32.4" y_offset="418.56" spacing="-0.11">illusion of hearing and judge the different words as </line>
      <line x_offset="32.4" y_offset="407.64" spacing="-0.11">same one. </line>
      <line x_offset="32.4" y_offset="396.0" spacing="0.61">Meaning: MIO words have many synonyms and </line>
      <line x_offset="14.4" y_offset="395.23" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="385.08" spacing="-0.87">much assonance. </line>
      <line x_offset="32.4" y_offset="373.44" spacing="0.61">Situation: Some are only used in specific situation. </line>
      <line x_offset="14.4" y_offset="372.67" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="362.52" spacing="-0.87">For example, "jime jime" means muggy, dump and </line>
      <line x_offset="32.4" y_offset="351.6" spacing="-0.11">humid, but it almost be used only in a rainy season. </line>
      <line x_offset="14.4" y_offset="340.68" spacing="-0.11">Most of the MIO words are used to describe the speaker's </line>
      <line x_offset="0.0" y_offset="329.64" spacing="0.01">feeling. In order to know the speaker's feeling, we attempt to </line>
      <line x_offset="0.0" y_offset="318.72" spacing="-0.11">acquire user's context with sensor. </line>
      <line x_offset="135.6" y_offset="299.4" spacing="8.29"> 1.0 A 2.0 ND </line>
      <line x_offset="62.04" y_offset="299.4" spacing="-11.16">III. JAMIOLAS </line>
      <line x_offset="14.4" y_offset="284.4" spacing="3.97">JAMIOLAS 1.0 is implemented by wearable sensors </line>
      <line x_offset="0.0" y_offset="273.48" spacing="-0.11">called Phidgets (physical widgets) [8] and a Tablet PC (HP </line>
      <line x_offset="0.0" y_offset="262.56" spacing="-0.11">T1100) . When learning, the learner must wear Phidgets </line>
      <line x_offset="0.0" y_offset="251.52" spacing="0.01">connected to the system, and select a MIO as answer that is </line>
      <line x_offset="0.0" y_offset="240.6" spacing="-0.11">most suitable for the situation in the question generated by </line>
      <line x_offset="0.0" y_offset="229.68" spacing="-0.11">system. However, when learning, sometimes learners do not </line>
      <line x_offset="0.0" y_offset="218.76" spacing="-0.11">know where he/she could learn the MIO. Learner must carry </line>
      <line x_offset="0.0" y_offset="207.84" spacing="-0.11">the system when using it, so it is not so convenient. </line>
      <line x_offset="14.4" y_offset="196.92" spacing="-0.11">JAMIOLAS 2.0 use the wireless sensor network instead </line>
      <line x_offset="0.0" y_offset="186.0" spacing="-0.11">of wearable sensor, and use RFID to recognize user's </line>
      <line x_offset="0.0" y_offset="175.08" spacing="-0.11">position. However, most of MIO words cannot be supported </line>
      <line x_offset="0.0" y_offset="164.16" spacing="-0.11">by it, and it can only be used in limited area. For these issues, </line>
      <line x_offset="0.0" y_offset="153.24" spacing="-0.11">we propose JAMIOLAS 3.0 to support learning MIO. </line>
      <line x_offset="153.24" y_offset="133.92" spacing="8.29"> 3.0 </line>
      <line x_offset="79.44" y_offset="133.92" spacing="-11.16">IV. JAMIOLAS </line>
      <line x_offset="0.0" y_offset="112.44" spacing="10.45">A. Context and sensor </line>
      <line x_offset="14.4" y_offset="98.28" spacing="3.13">There are three important aspect of context: Where you </line>
      <line x_offset="0.0" y_offset="87.36" spacing="-0.11">are, who you are with, and what resources are nearby [9]. </line>
      <line x_offset="0.0" y_offset="76.44" spacing="-0.11">Context includes not only user's location, but also the </line>
      <line x_offset="0.0" y_offset="65.52" spacing="-0.11">lighting, noise level, social situation and so on [10]. Human </line>
      <line x_offset="0.0" y_offset="54.6" spacing="-0.11">being usually gets the feeling from environment by five </line>
      <line x_offset="0.0" y_offset="43.68" spacing="-0.11">senses including seeing, hearing, smelling, tasting and </line>
      <line x_offset="0.0" y_offset="32.76" spacing="-0.11">touching. It is possible to get such context with sensors. The </line>
      <line x_offset="0.0" y_offset="21.84" spacing="-0.11">context can be classified as two types - can be created by </line>
      <line x_offset="0.0" y_offset="10.92" spacing="-0.11">computer (scene, sound) and cannot be created by computer </line>
      <line x_offset="0.0" y_offset="0.0" spacing="-0.11">(weather). </line>
    </region>
    <region x="300.0" y="43.28" width="12.0" height="7.26" line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">117</line>
    </region>
    <region x="50.0" y="33.28" width="142.25" height="17.26" line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">
      <line x_offset="0.0" y_offset="10.0" spacing="0.0">978-0-7695-3992-8/10 $26.00 ' 2010 IEEE</line>
      <line x_offset="0.0" y_offset="0.0" spacing="2.74">DOI 10.1109/WMUTE.2010.24</line>
    </region>
  </page>
  <page width="612" height="792" number="2">
    <region x="62.76" y="708.9" width="34.28" height="8.9" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">TABLE I. </line>
    </region>
    <region x="97.2" y="700.96" width="192.54" height="16.96" line_height="7.17" font="OCAONP+TimesNewRoman">
      <line x_offset="141.36" y_offset="7.94" spacing="0.11">,</line>
      <line x_offset="0.0" y_offset="7.94" spacing="-9.01"> RELATIONSHIP BETWEEN BODY SENSE CONTEXT AND </line>
      <line x_offset="65.76" y_offset="0.0" spacing="0.77">SENSOR</line>
    </region>
    <region x="56.28" y="667.74" width="60.24" height="23.66" line_height="8.9" font="OCAONN+TimesNewRoman,Bold">
      <line x_offset="19.32" y_offset="14.76" spacing="0.0">Body Sense </line>
      <line x_offset="0.0" y_offset="0.0" spacing="5.86">Seeing </line>
    </region>
    <region x="56.28" y="622.26" width="27.84" height="24.26" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="15.36" spacing="0.0">Hearing </line>
      <line x_offset="0.0" y_offset="0.0" spacing="6.46">Feeling </line>
    </region>
    <region x="139.15" y="622.26" width="54.46" height="69.14" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="25.31" y_offset="60.24" spacing="0.0">Context </line>
      <line x_offset="0.13" y_offset="45.48" spacing="5.86">Light </line>
      <line x_offset="0.13" y_offset="30.72" spacing="5.86">Scene </line>
      <line x_offset="0.02" y_offset="15.36" spacing="6.46">Sound </line>
      <line x_offset="0.0" y_offset="0.0" spacing="6.46">Temperature </line>
    </region>
    <region x="222.15" y="622.26" width="66.69" height="69.14" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="23.84" y_offset="60.24" spacing="0.0">Sensor </line>
      <line x_offset="0.25" y_offset="45.48" spacing="5.86">Light Sensor </line>
      <line x_offset="0.27" y_offset="30.72" spacing="5.86">Image Sensor </line>
      <line x_offset="0.0" y_offset="15.36" spacing="6.46">Sound Sensor </line>
      <line x_offset="0.01" y_offset="0.0" spacing="6.46">Temperature Sensor </line>
    </region>
    <region x="344.64" y="602.41" width="200.11" height="11.03" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">(A)Quiz for context (B)Give a wrong answer </line>
    </region>
    <region x="54.0" y="511.09" width="246.28" height="101.63" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="90.6" spacing="0.0">B. Implementation </line>
      <line x_offset="14.4" y_offset="76.44" spacing="3.13">Figure 1 shows the architecture of system. The weather </line>
      <line x_offset="0.0" y_offset="65.52" spacing="-0.11">information and media files are learning stuff in this system. </line>
      <line x_offset="0.0" y_offset="54.6" spacing="-0.11">We are using real-time on-line weather service as sensor </line>
      <line x_offset="0.0" y_offset="43.68" spacing="-0.11">network. As the feeling is different one by one, the system </line>
      <line x_offset="0.0" y_offset="32.76" spacing="-0.11">will use the voting mode to decide the proximate select to the </line>
      <line x_offset="0.0" y_offset="21.84" spacing="-0.11">weather or media. We plan to use mobile as client, but for </line>
      <line x_offset="0.0" y_offset="10.92" spacing="-0.11">limitation on condition, we have to implement this system on </line>
      <line x_offset="0.0" y_offset="0.0" spacing="-0.11">web side at current. </line>
    </region>
    <region x="339.12" y="473.82" width="196.63" height="24.17" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="13.14" spacing="0.0">(D)Word test </line>
      <line x_offset="71.89" y_offset="13.14" spacing="-11.03"> (C)Give a right answer </line>
      <line x_offset="49.8" y_offset="0.0" spacing="4.24">Figure 2. Student's interface </line>
    </region>
    <region x="125.76" y="361.38" width="101.47" height="8.9" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">Figure 1. System architecture </line>
    </region>
    <region x="54.0" y="130.93" width="246.12" height="214.19" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="203.16" spacing="0.0">C. System interface and function </line>
      <line x_offset="9.0" y_offset="188.04" spacing="4.09">1) Student's interface </line>
      <line x_offset="14.4" y_offset="177.0" spacing="0.01">This system mainly supports the following functions for </line>
      <line x_offset="0.0" y_offset="166.08" spacing="-0.11">students. </line>
      <line x_offset="32.4" y_offset="154.44" spacing="0.61">Learning by weather information: There are two </line>
      <line x_offset="14.4" y_offset="153.67" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="143.52" spacing="-0.87">modes: Fix mode - system will show uses user's </line>
      <line x_offset="32.4" y_offset="132.6" spacing="-0.11">default location, and generate a quiz to ask learner </line>
      <line x_offset="32.4" y_offset="121.68" spacing="-0.11">how to describe the current weather with MIO; Tour </line>
      <line x_offset="32.4" y_offset="110.64" spacing="0.01">mode - learner must set the coordinate first. When </line>
      <line x_offset="32.4" y_offset="99.72" spacing="-0.11">learner gave a right answer, he/she can view the </line>
      <line x_offset="32.4" y_offset="88.8" spacing="-0.11">example and media for each selection or enter the </line>
      <line x_offset="32.4" y_offset="77.88" spacing="-0.11">test mode to take a test. </line>
      <line x_offset="32.4" y_offset="66.24" spacing="0.61">Learning by media: Learner choose a media file and </line>
      <line x_offset="14.4" y_offset="65.47" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="55.32" spacing="-0.87">give select a MIO word that is most suitable for this </line>
      <line x_offset="32.4" y_offset="44.4" spacing="-0.11">media. Finally there is a test function for learner </line>
      <line x_offset="32.4" y_offset="32.76" spacing="0.61">Learning in free mode: the free mode likes a media </line>
      <line x_offset="14.4" y_offset="31.99" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="21.84" spacing="-0.87">dictionary, user can look up a word and the result is </line>
      <line x_offset="32.4" y_offset="10.92" spacing="-0.11">composed of examples and media. </line>
      <line x_offset="14.4" y_offset="0.0" spacing="-0.11">Figure 2 shows a typical learning flow in this system. </line>
    </region>
    <region x="315.0" y="122.29" width="246.21" height="340.67" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="9.0" y_offset="329.64" spacing="0.0">2) Teacher's interface </line>
      <line x_offset="14.4" y_offset="318.6" spacing="0.01">The teacher can use all the functions available for </line>
      <line x_offset="0.0" y_offset="307.68" spacing="-0.11">students. In addition, the teacher has access to another two </line>
      <line x_offset="0.0" y_offset="296.76" spacing="-0.11">functions. </line>
      <line x_offset="32.4" y_offset="285.12" spacing="0.61">Evaluate weather/media: The teacher can vote a </line>
      <line x_offset="14.4" y_offset="284.35" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="274.2" spacing="-0.87">word for weather/media. There are some restrictions </line>
      <line x_offset="32.4" y_offset="263.16" spacing="0.01">for evaluation. The user who has the role of teacher </line>
      <line x_offset="32.4" y_offset="252.24" spacing="-0.11">can only vote for the weather once within one hour </line>
      <line x_offset="32.4" y_offset="241.32" spacing="-0.11">unless the weather information has been updated. </line>
      <line x_offset="32.4" y_offset="230.4" spacing="-0.11">And each media file can only be voted for once by </line>
      <line x_offset="32.4" y_offset="219.48" spacing="-0.11">each user. </line>
      <line x_offset="32.4" y_offset="207.84" spacing="0.61">Word management: The user who has the role of </line>
      <line x_offset="14.4" y_offset="207.07" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="196.92" spacing="-0.87">teacher can manage words. This function contains </line>
      <line x_offset="32.4" y_offset="186.0" spacing="-0.11">the common CRUD (create, retrieve, update, delete), </line>
      <line x_offset="32.4" y_offset="175.08" spacing="-0.11">word example management, and weather word </line>
      <line x_offset="32.4" y_offset="164.16" spacing="-0.11">management. </line>
      <line x_offset="115.44" y_offset="145.26" spacing="10.0">VALUATION</line>
      <line x_offset="86.4" y_offset="144.84" spacing="-10.61">V. E</line>
      <line x_offset="0.0" y_offset="123.36" spacing="10.45">A. Method </line>
      <line x_offset="14.4" y_offset="109.32" spacing="3.01">We have done an experiment to compare JAMIOALS 3.0 </line>
      <line x_offset="0.0" y_offset="98.28" spacing="0.01">to traditional method. After voting by Japanese students, we </line>
      <line x_offset="0.0" y_offset="87.36" spacing="-0.11">planned to prepare 10 words as test data in this experiment </line>
      <line x_offset="0.0" y_offset="76.44" spacing="-0.11">(Table II). 6 Japanese learners took the part of student. In </line>
      <line x_offset="0.0" y_offset="65.52" spacing="-0.11">them, 1 is living in Tokushima (Japan), 3 are living in Tokyo </line>
      <line x_offset="0.0" y_offset="54.6" spacing="-0.11">(Japan) and 2 are living in Dalian (China). They were </line>
      <line x_offset="0.0" y_offset="43.68" spacing="-0.11">divided into two groups: one used dictionary first, and </line>
      <line x_offset="0.0" y_offset="32.76" spacing="-0.11">another used system. After 20 minutes, they exchanged, and </line>
      <line x_offset="0.0" y_offset="21.84" spacing="-0.11">continue to learn for 20 minutes. We put pre-test, mid-test </line>
      <line x_offset="0.0" y_offset="10.92" spacing="-0.11">and post-test in this experiment. Finally, they answered a </line>
      <line x_offset="0.0" y_offset="0.0" spacing="-0.11">questionnaire that is 5 ranges from 1 to 5. </line>
    </region>
    <region x="300.0" y="43.28" width="12.0" height="7.26" line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">118</line>
    </region>
  </page>
  <page width="612" height="792" number="3">
    <region x="88.56" y="708.9" width="37.04" height="8.9" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">TABLE II. </line>
    </region>
    <region x="142.56" y="708.9" width="119.76" height="8.9" line_height="7.17" font="OCAONP+TimesNewRoman">
      <line x_offset="15.48" y_offset="0.34" spacing="1.39"> USED IN THE EXPERIMENTATION</line>
      <line x_offset="0.0" y_offset="0.0" spacing="-8.56">MIO</line>
    </region>
    <region x="491.88" y="645.43" width="48.14" height="19.76" line_height="8.24" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="11.52" spacing="0.0">Dictionary</line>
      <line x_offset="0.0" y_offset="0.0" spacing="3.28">JAMIOLAS 3.0</line>
    </region>
    <region x="57.48" y="637.62" width="238.69" height="62.06" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="29.4" y_offset="53.16" spacing="0.0">Context </line>
      <line x_offset="155.34" y_offset="53.16" spacing="-8.9">MIO </line>
      <line x_offset="0.0" y_offset="38.64" spacing="5.62">Weather(Temperature) </line>
      <line x_offset="91.49" y_offset="38.64" spacing="-8.9">hinyari, nuku nuku </line>
      <line x_offset="0.0" y_offset="23.88" spacing="5.86">Scene(Posture of walking) uro uro, tyoko tyoko, noshi noshi, yochi </line>
      <line x_offset="91.44" y_offset="14.64" spacing="0.34">yochi </line>
      <line x_offset="0.0" y_offset="0.0" spacing="5.74">Sound(Sound of animals) ka ka, ga ga, gero gero, tyun tyun </line>
    </region>
    <region x="371.64" y="579.9" width="132.08" height="9.01" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">Figure 4. Total increased score on type </line>
    </region>
    <region x="54.0" y="514.57" width="246.07" height="102.71" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="91.68" spacing="0.0">B. Result </line>
      <line x_offset="14.4" y_offset="77.52" spacing="3.13">We did three tests for each learner during the </line>
      <line x_offset="0.0" y_offset="66.6" spacing="-0.11">experiments and administered a questionnaire. These </line>
      <line x_offset="0.0" y_offset="55.68" spacing="-0.11">supplied us with our experimental data. Although this is an </line>
      <line x_offset="0.0" y_offset="44.76" spacing="-0.11">initial experiment, with the analysis of the experimental data </line>
      <line x_offset="0.0" y_offset="33.84" spacing="-0.11">we can see the following results. </line>
      <line x_offset="9.0" y_offset="21.96" spacing="0.85">1) The overall results </line>
      <line x_offset="14.4" y_offset="10.92" spacing="0.01">The following chart (Figure 3) shows the results of the </line>
      <line x_offset="0.0" y_offset="0.0" spacing="-0.11">pre-test, mid-test and post-test. </line>
    </region>
    <region x="237.12" y="425.12" width="40.49" height="30.5" line_height="6.98" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="23.52" spacing="0.0">Dictionary First</line>
      <line x_offset="0.0" y_offset="16.2" spacing="0.34">Group</line>
      <line x_offset="0.0" y_offset="7.32" spacing="1.9">System First</line>
      <line x_offset="0.0" y_offset="0.0" spacing="0.34">Group</line>
    </region>
    <region x="100.32" y="396.65" width="21.52" height="7.73" line_height="7.73" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">Pre-test</line>
    </region>
    <region x="179.78" y="396.65" width="24.25" height="7.73" line_height="7.73" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">Post-test</line>
    </region>
    <region x="139.57" y="396.65" width="23.92" height="7.73" line_height="7.73" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">Mid-test</line>
    </region>
    <region x="89.4" y="373.14" width="174.54" height="8.9" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">Figure 3. Average increased score in the experiment </line>
    </region>
    <region x="315.0" y="284.77" width="246.2" height="285.23" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="14.4" y_offset="274.2" spacing="0.0">Figure 4 shows the increase in score for each type of </line>
      <line x_offset="0.0" y_offset="263.28" spacing="-0.11">context. All three types were learned better by using the </line>
      <line x_offset="0.0" y_offset="252.36" spacing="-0.11">system than by using the dictionary. It shows that words </line>
      <line x_offset="0.0" y_offset="241.44" spacing="-0.11">related to sound (onomatopoeia) showed the highest increase </line>
      <line x_offset="0.0" y_offset="230.52" spacing="-0.11">in score both by the dictionary and the system. For the </line>
      <line x_offset="0.0" y_offset="219.6" spacing="-0.11">weather context, only the system provided benefit; in this </line>
      <line x_offset="0.0" y_offset="208.68" spacing="-0.11">experiment, the dictionary had no effect on learning the </line>
      <line x_offset="0.0" y_offset="197.76" spacing="-0.11">words related to weather. On the contrary, one learner </line>
      <line x_offset="0.0" y_offset="186.72" spacing="0.01">changed their original right answer into a wrong answer after </line>
      <line x_offset="0.0" y_offset="175.8" spacing="-0.11">learning the weather words using the dictionary and got a </line>
      <line x_offset="0.0" y_offset="164.88" spacing="-0.11">lower score. This suggests that the dictionary cannot </line>
      <line x_offset="0.0" y_offset="153.96" spacing="-0.11">effectively explain MIO related to weather. </line>
      <line x_offset="9.0" y_offset="142.08" spacing="0.85">3) Questionnaire </line>
      <line x_offset="14.4" y_offset="131.04" spacing="0.01">Table III shows the results of the questionnaire. </line>
      <line x_offset="0.0" y_offset="120.12" spacing="-0.11">According to Question (1), we learned that the system is </line>
      <line x_offset="0.0" y_offset="109.2" spacing="-0.11">helpful for learning mimicry and onomatopoeia. The highest </line>
      <line x_offset="0.0" y_offset="98.28" spacing="-0.11">average score and lowest standard deviation of Question (1) </line>
      <line x_offset="0.0" y_offset="87.36" spacing="-0.11">in the questionnaire shows the system is generally </line>
      <line x_offset="0.0" y_offset="76.44" spacing="-0.11">considered useful. Question (2) asked whether the answers of </line>
      <line x_offset="0.0" y_offset="65.52" spacing="-0.11">the presented quiz were appropriate to the situation or not. In </line>
      <line x_offset="0.0" y_offset="54.6" spacing="-0.11">this case the result was less satisfactory, it demonstrates the </line>
      <line x_offset="0.0" y_offset="43.68" spacing="-0.11">biggest problem of learning MIO is with feelings, because by </line>
      <line x_offset="0.0" y_offset="32.76" spacing="-0.11">the standard deviation we can see the feeling is different </line>
      <line x_offset="0.0" y_offset="21.84" spacing="-0.11">depending upon the person. Therefore, in the future we </line>
      <line x_offset="0.0" y_offset="10.92" spacing="-0.11">should seek for a method to make the words more </line>
      <line x_offset="0.0" y_offset="0.0" spacing="-0.11">appropriate to the situation. </line>
    </region>
    <region x="364.68" y="264.42" width="39.68" height="8.9" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">TABLE III. </line>
    </region>
    <region x="418.68" y="264.42" width="89.4" height="8.9" line_height="7.17" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">RESULT OF QUESTIONNAIRE</line>
    </region>
    <region x="315.0" y="121.38" width="243.4" height="133.82" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="93.4" y_offset="124.92" spacing="0.0">Question </line>
      <line x_offset="213.53" y_offset="124.92" spacing="-8.9">Avg SD</line>
      <line x_offset="0.0" y_offset="110.28" spacing="5.74">1 Were you able to learn mimicry and onomatopoeia by this 4.8 0.4</line>
      <line x_offset="9.48" y_offset="101.16" spacing="0.22">system? </line>
      <line x_offset="0.0" y_offset="86.4" spacing="5.86">2 Was the answer of the presented quiz appropriate to the 3.8 0.8</line>
      <line x_offset="9.48" y_offset="77.28" spacing="0.22">situation? </line>
      <line x_offset="0.0" y_offset="62.52" spacing="5.86">3 Were you able to learn mimicry and onomatopoeia with 4.2 0.8</line>
      <line x_offset="9.48" y_offset="53.28" spacing="0.34">enjoyment? </line>
      <line x_offset="0.0" y_offset="38.64" spacing="5.74">4 Is the system easy to use? </line>
      <line x_offset="214.0" y_offset="38.64" spacing="-8.9">4.2 0.8</line>
      <line x_offset="0.0" y_offset="23.88" spacing="5.86">5 Do you want to learn by using this system in the future? </line>
      <line x_offset="212.88" y_offset="23.88" spacing="-8.9">4.3 0.5</line>
      <line x_offset="0.0" y_offset="9.24" spacing="5.74">6 Which do you think enhances learning more, this system or 4.5 0.5</line>
      <line x_offset="9.48" y_offset="0.0" spacing="0.34">traditional learning? (this system: 5, traditional learning: 1) </line>
    </region>
    <region x="54.0" y="110.77" width="246.14" height="252.35" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="14.4" y_offset="241.32" spacing="0.0">Figure 3 shows the average score increases in the mid-</line>
      <line x_offset="0.0" y_offset="230.4" spacing="-0.11">test and post-test. As shown in this figure, the system first </line>
      <line x_offset="0.0" y_offset="219.48" spacing="-0.11">group showed better increased score than the dictionary first </line>
      <line x_offset="0.0" y_offset="208.56" spacing="-0.11">group. After the mid-test and the group changeover, the final </line>
      <line x_offset="0.0" y_offset="197.64" spacing="-0.11">results from the system first group were higher than for the </line>
      <line x_offset="0.0" y_offset="186.72" spacing="-0.11">dictionary first group. Although the data from this small </line>
      <line x_offset="0.0" y_offset="175.8" spacing="-0.11">sample size would not bear statistical analysis, the results </line>
      <line x_offset="0.0" y_offset="164.88" spacing="-0.11">suggest that JAMIOLAS 3.0 can be more effective than a </line>
      <line x_offset="0.0" y_offset="153.96" spacing="-0.11">dictionary for learning MIO. </line>
      <line x_offset="14.4" y_offset="143.04" spacing="-0.11">From feedback, we learned that when users in the system </line>
      <line x_offset="0.0" y_offset="132.12" spacing="-0.11">first group used JAMIOLAS 3.0 from the beginning, both </line>
      <line x_offset="0.0" y_offset="121.2" spacing="-0.11">the system and the words were new to them, and they made </line>
      <line x_offset="0.0" y_offset="110.28" spacing="-0.11">full use of system. However, when users had first learned the </line>
      <line x_offset="0.0" y_offset="99.36" spacing="-0.11">meaning of words from the dictionary, they lost the freshness </line>
      <line x_offset="0.0" y_offset="88.44" spacing="-0.11">of the words, so they felt that they had already understood </line>
      <line x_offset="0.0" y_offset="77.52" spacing="-0.11">the meaning and the system was underutilized. In the </line>
      <line x_offset="0.0" y_offset="66.6" spacing="-0.11">questionnaire, most learners suggested we add the meanings </line>
      <line x_offset="0.0" y_offset="55.68" spacing="-0.11">in the system, and the system should provide an explanation </line>
      <line x_offset="0.0" y_offset="44.76" spacing="-0.11">when the results are shown. </line>
      <line x_offset="9.0" y_offset="32.88" spacing="0.85">2) Results on type of context </line>
      <line x_offset="14.4" y_offset="21.84" spacing="0.01">There were three types of context in the evaluation </line>
      <line x_offset="0.0" y_offset="10.92" spacing="-0.11">experiment. We used the mid-test data to plot this in order to </line>
      <line x_offset="0.0" y_offset="0.0" spacing="-0.11">see the effects of the systems for different types of context. </line>
    </region>
    <region x="315.0" y="74.65" width="245.98" height="32.87" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="14.4" y_offset="21.84" spacing="0.0">Results to Question (3) showed learners were able to </line>
      <line x_offset="0.0" y_offset="10.92" spacing="-0.11">effectively learn using this system, which can motivate them </line>
      <line x_offset="0.0" y_offset="0.0" spacing="-0.11">to learn language. For Question (4), most learners thought </line>
    </region>
    <region x="300.0" y="43.28" width="12.0" height="7.26" line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">119</line>
    </region>
  </page>
  <page width="612" height="792" number="4">
    <region x="315.0" y="412.8" width="248.54" height="307.89" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="32.4" y_offset="295.68" spacing="1.19">Using embedded mobile sensors to acquire sensor </line>
      <line x_offset="14.4" y_offset="294.92" spacing="-12.21">&#x2022; </line>
      <line x_offset="32.4" y_offset="284.76" spacing="-0.87">data anywhere at any time. The current system is not </line>
      <line x_offset="32.4" y_offset="273.84" spacing="-0.11">using sensors that we have deployed, but the sensor </line>
      <line x_offset="32.4" y_offset="262.92" spacing="-0.11">data is received from the global sensor network that </line>
      <line x_offset="32.4" y_offset="252.0" spacing="-0.11">is supplied by an on-line weather provider. In the </line>
      <line x_offset="32.4" y_offset="241.08" spacing="-0.11">future, sensors will be embedded in each mobile, as </line>
      <line x_offset="32.4" y_offset="230.16" spacing="-0.11">users' phones will know a lot about the world around </line>
      <line x_offset="32.4" y_offset="219.24" spacing="-0.11">them [11,12]. Our future plan is to support Japanese </line>
      <line x_offset="32.4" y_offset="208.32" spacing="-0.11">MIO learning with the sensors that have been </line>
      <line x_offset="32.4" y_offset="197.4" spacing="-0.11">embedded in the next generation of mobile </line>
      <line x_offset="32.4" y_offset="186.48" spacing="-0.11">technology, especially mobiles using the Android </line>
      <line x_offset="32.4" y_offset="175.56" spacing="-0.11">operating system [13]. Android has already supplied </line>
      <line x_offset="32.4" y_offset="164.64" spacing="-0.11">an API for 8 types of sensor including accelerometer, </line>
      <line x_offset="32.4" y_offset="153.72" spacing="-0.11">gyroscope, light, magnetic field, orientation, </line>
      <line x_offset="32.4" y_offset="142.8" spacing="-0.11">pressure, proximity and temperature sensors [14,15]. </line>
      <line x_offset="32.4" y_offset="131.16" spacing="0.61">Introducing biosensors to support learning MIO </line>
      <line x_offset="14.4" y_offset="130.39" spacing="-12.21">&#x2022;</line>
      <line x_offset="32.4" y_offset="120.12" spacing="-0.75">related to emotion. Because most MIO are just used </line>
      <line x_offset="32.4" y_offset="109.2" spacing="-0.11">to describe one's feelings, there are many words </line>
      <line x_offset="32.4" y_offset="98.28" spacing="-0.11">related to emotion, but these cannot be supported by </line>
      <line x_offset="32.4" y_offset="87.36" spacing="-0.11">the current system. One way to recognize the </line>
      <line x_offset="32.4" y_offset="76.44" spacing="-0.11">emotion is using biosensors to acquire information </line>
      <line x_offset="32.4" y_offset="65.52" spacing="-0.11">from the human body [16]. </line>
      <line x_offset="14.4" y_offset="54.6" spacing="-0.11">In addition, we must look for an approach to making the </line>
      <line x_offset="0.0" y_offset="43.68" spacing="-0.11">questions in the system more appropriate to the learning </line>
      <line x_offset="0.0" y_offset="32.76" spacing="-0.11">context. We should also consider how to introduce the </line>
      <line x_offset="0.0" y_offset="21.84" spacing="-0.11">meaning of words into the system more effectively. </line>
      <line x_offset="0.0" y_offset="10.92" spacing="-0.11">However, we will improve the system and conduct further </line>
      <line x_offset="0.0" y_offset="0.0" spacing="-0.11">evaluations to gather more experimental data. </line>
    </region>
    <region x="329.16" y="82.5" width="231.73" height="319.22" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="310.32" spacing="0.0">[1] G.D. Abowd and E.D. Mynatt, "Charting past, present, and future </line>
      <line x_offset="18.0" y_offset="301.32" spacing="0.1">research in ubiquitous computing," ACM Transactions on </line>
      <line x_offset="18.0" y_offset="292.44" spacing="-0.02">Computer-Human Interaction (TOCHI), vol. 7, 2000, pp. 29-58. </line>
      <line x_offset="0.0" y_offset="280.8" spacing="2.74">[2] G. Fischer, "User Modeling in Human-Computer Interaction," </line>
      <line x_offset="18.0" y_offset="271.8" spacing="0.1">User Modeling and User-Adapted Interaction, vol. 11, 2001, pp. </line>
      <line x_offset="18.0" y_offset="262.92" spacing="-0.02">65-86. </line>
      <line x_offset="0.0" y_offset="251.28" spacing="2.74">[3] Y.S. Chen, T.C. Kao, J.P. Sheu, and C.Y. Chiang, "A mobile </line>
      <line x_offset="18.0" y_offset="242.28" spacing="0.1">scaffolding-aid-based bird-watching learning system," 2002, pp. </line>
      <line x_offset="18.0" y_offset="233.4" spacing="-0.02">15-22. </line>
      <line x_offset="0.0" y_offset="221.88" spacing="2.62">[4] H. Ogata and Y. Yano, "Context-aware support for computer-</line>
      <line x_offset="18.0" y_offset="213.0" spacing="-0.02">supported ubiquitous learning," 2004, pp. 27-34. </line>
      <line x_offset="0.0" y_offset="201.36" spacing="2.74">[5] M. Miyata, H. Ogata, T. Kondo, and Y. Yano, "JAMIOLAS 2.0: </line>
      <line x_offset="18.0" y_offset="192.36" spacing="0.1">Supporting to Learn Japanese Mimetic Words and Onomatopoeia </line>
      <line x_offset="18.0" y_offset="183.48" spacing="-0.02">with Wireless Sensor Networks," Taipei: 2008, pp. 643-650. </line>
      <line x_offset="0.0" y_offset="171.84" spacing="2.74">[6] H. Ogata, T. Kondo, C. Yina, Y. Liub, and Y. Yanoa, "Computer </line>
      <line x_offset="18.0" y_offset="162.84" spacing="0.1">Supported Ubiquitous Learning Environment for Japanese </line>
      <line x_offset="18.0" y_offset="153.84" spacing="0.1">Mimicry and Onomatopoeia with Sensors," Supporting Learning </line>
      <line x_offset="18.0" y_offset="144.96" spacing="-0.02">Flow Through Integrative Technologies, 2007, p. 463. </line>
      <line x_offset="0.0" y_offset="133.32" spacing="2.74">[7] H. Ogata, C. Yin, and Y. Yano, "JAMIOLAS: Supporting </line>
      <line x_offset="18.0" y_offset="124.32" spacing="0.1">Japanese Mimicry and Onomatopoeia Learning with Sensors," </line>
      <line x_offset="18.0" y_offset="115.44" spacing="-0.02">2006, pp. 111-115. </line>
      <line x_offset="0.0" y_offset="103.8" spacing="2.74">[8] S. Greenberg and C. Fitchett, "Phidgets: easy development of </line>
      <line x_offset="18.0" y_offset="94.8" spacing="0.1">physical interfaces through physical widgets," ACM Press, 2001, </line>
      <line x_offset="18.0" y_offset="85.92" spacing="-0.02">pp. 209-218. </line>
      <line x_offset="0.0" y_offset="74.28" spacing="2.74">[9] J. Lee, S. Oh, and M. Jeon, "A New Context-Aware Learning </line>
      <line x_offset="18.0" y_offset="65.28" spacing="0.1">System for Predicting Services to Users in Ubiquitous </line>
      <line x_offset="18.0" y_offset="56.28" spacing="0.1">Environment," International Symposium on Ubiquitous VR, </line>
      <line x_offset="18.0" y_offset="47.4" spacing="-0.02">2007, p. 1. </line>
      <line x_offset="0.0" y_offset="35.88" spacing="2.62">[10] B. Schilit, N. Adams, and R. Want, "Context-aware computing </line>
      <line x_offset="18.0" y_offset="26.88" spacing="0.1">applications," Mobile Computing Systems and Applications, </line>
      <line x_offset="18.0" y_offset="17.88" spacing="0.1">1994. WMCSA'08. First Workshop on on Mobile Computing </line>
      <line x_offset="18.0" y_offset="8.88" spacing="0.1">Systems and Applications, Santa Cruz, CA, USA: 1994, pp. 85-</line>
      <line x_offset="18.0" y_offset="0.0" spacing="-0.02">90. </line>
    </region>
    <region x="54.0" y="73.93" width="246.26" height="646.31" line_height="11.03" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="635.28" spacing="0.0">this system was easy to use. Question (3) and Question (4) </line>
      <line x_offset="0.0" y_offset="624.36" spacing="-0.11">also have high standard deviations, and these tell us that we </line>
      <line x_offset="0.0" y_offset="613.44" spacing="-0.11">must improve the interface of system and provide better </line>
      <line x_offset="0.0" y_offset="602.52" spacing="-0.11">learning experience. Question (5) shows that users like using </line>
      <line x_offset="0.0" y_offset="591.6" spacing="-0.11">this kind of system and would use it again. Question (6) </line>
      <line x_offset="0.0" y_offset="580.68" spacing="-0.11">shows that compared to traditional learning; most learners </line>
      <line x_offset="0.0" y_offset="569.76" spacing="-0.11">thought this system was more effective for learning MIO. </line>
      <line x_offset="14.4" y_offset="558.84" spacing="-0.11">Apart from the above questions, we also asked for </line>
      <line x_offset="0.0" y_offset="547.92" spacing="-0.11">feedback regarding the difficulties users encountered as they </line>
      <line x_offset="0.0" y_offset="537.0" spacing="-0.11">were learning MIO. All of them said the words were difficult </line>
      <line x_offset="0.0" y_offset="526.08" spacing="-0.11">to remember, but two of the users could not explain why. </line>
      <line x_offset="0.0" y_offset="515.16" spacing="-0.11">Others said there are too many MIO words, and they could </line>
      <line x_offset="0.0" y_offset="504.24" spacing="-0.11">not guess the meaning using the limited information. In </line>
      <line x_offset="0.0" y_offset="493.32" spacing="-0.11">addition, although users had learned the meaning, because </line>
      <line x_offset="0.0" y_offset="482.4" spacing="-0.11">they had no intuitive feeling and no environment, it was easy </line>
      <line x_offset="0.0" y_offset="471.36" spacing="0.01">to forget MIO words. These answers will help us to improve </line>
      <line x_offset="0.0" y_offset="460.44" spacing="-0.11">the system in future work. We also asked the learners to </line>
      <line x_offset="0.0" y_offset="449.52" spacing="-0.11">make some general comments on the system. One user said </line>
      <line x_offset="0.0" y_offset="438.6" spacing="-0.11">this system was very interesting, they could easily learn the </line>
      <line x_offset="0.0" y_offset="427.68" spacing="-0.11">meaning the words in an environment, and it is a really good </line>
      <line x_offset="0.0" y_offset="416.76" spacing="-0.11">way to explain the MIO words by feeling. Most of users said </line>
      <line x_offset="0.0" y_offset="405.84" spacing="-0.11">when the answer of a quiz is given, they could only see what </line>
      <line x_offset="0.0" y_offset="394.92" spacing="-0.11">was right or wrong, but could not learn why. If the meaning </line>
      <line x_offset="0.0" y_offset="384.0" spacing="-0.11">could be shown at that time, it will be more conducive to </line>
      <line x_offset="0.0" y_offset="373.08" spacing="-0.11">remembering the words. One user said that in the test there </line>
      <line x_offset="0.0" y_offset="362.16" spacing="-0.11">were sometimes too many questions for one word. This is in </line>
      <line x_offset="0.0" y_offset="351.24" spacing="-0.11">fact a result of the current algorithm, which we can improve </line>
      <line x_offset="0.0" y_offset="340.32" spacing="-0.11">in the future. Another user also said the video was not so </line>
      <line x_offset="0.0" y_offset="329.4" spacing="-0.11">clear; however this problem cannot be solved at present </line>
      <line x_offset="0.0" y_offset="318.48" spacing="-0.11">because of the limitation of technology. </line>
      <line x_offset="14.4" y_offset="307.56" spacing="-0.11">During the evaluation, we learned that the system can </line>
      <line x_offset="0.0" y_offset="296.64" spacing="-0.11">explain the words that related to feeling very well by sensor </line>
      <line x_offset="0.0" y_offset="285.72" spacing="-0.11">data, and that learning efficacy and quality are better than </line>
      <line x_offset="0.0" y_offset="274.8" spacing="-0.11">traditional learning. </line>
      <line x_offset="45.48" y_offset="255.48" spacing="8.16">VI. C ONCLUSION AND FUTURE WORK </line>
      <line x_offset="14.4" y_offset="240.36" spacing="4.09">In this paper, we described an improved context-aware </line>
      <line x_offset="0.0" y_offset="229.44" spacing="-0.11">learning system named JAMIOLAS 3.0 that can support the </line>
      <line x_offset="0.0" y_offset="218.52" spacing="-0.11">learning of Japanese MIO words in a sensory learning mode </line>
      <line x_offset="0.0" y_offset="207.6" spacing="-0.11">with global sensor network. This is an on-line system that </line>
      <line x_offset="0.0" y_offset="196.68" spacing="-0.11">can be used in a broader area than previous versions. It uses </line>
      <line x_offset="0.0" y_offset="185.76" spacing="-0.11">on-line weather information as global weather sensor data. In </line>
      <line x_offset="0.0" y_offset="174.84" spacing="-0.11">addition, it also uses media (video/audio) as image/sound </line>
      <line x_offset="0.0" y_offset="163.92" spacing="-0.11">sensor data and can support learning more words than </line>
      <line x_offset="0.0" y_offset="153.0" spacing="-0.11">before. Two methods are used in this system: created context </line>
      <line x_offset="0.0" y_offset="142.08" spacing="-0.11">and detecting context automatically to provide the right MIO </line>
      <line x_offset="0.0" y_offset="131.16" spacing="-0.11">for the learner's context. Through an initial evaluation </line>
      <line x_offset="0.0" y_offset="120.24" spacing="-0.11">experiment, we can see that this system is effective for </line>
      <line x_offset="0.0" y_offset="109.32" spacing="-0.11">learning MIO. The feeling related to a weather situation can </line>
      <line x_offset="0.0" y_offset="98.4" spacing="-0.11">be explained by the system more effectively than by </line>
      <line x_offset="0.0" y_offset="87.48" spacing="-0.11">dictionary. However, on the media side, although the media </line>
      <line x_offset="0.0" y_offset="76.56" spacing="-0.11">can be explained by the system better than by the dictionary, </line>
      <line x_offset="0.0" y_offset="65.52" spacing="0.01">the results and feedback suggest that the dictionary is still </line>
      <line x_offset="0.0" y_offset="54.6" spacing="-0.11">useful. With the help of the dictionary, a learner can learn the </line>
      <line x_offset="0.0" y_offset="43.68" spacing="-0.11">words using both meaning and feeling. In the future, we will </line>
      <line x_offset="0.0" y_offset="32.76" spacing="-0.11">focus on making the questions more appropriate to </line>
      <line x_offset="0.0" y_offset="21.84" spacing="-0.11">environment and weather data, and we will also consider </line>
      <line x_offset="0.0" y_offset="10.92" spacing="-0.11">how to use multiple conditions of weather information. </line>
      <line x_offset="14.4" y_offset="0.0" spacing="-0.11">Our future research will be in the following directions: </line>
    </region>
    <region x="300.0" y="43.28" width="12.0" height="7.26" line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">120</line>
    </region>
  </page>
  <page width="612" height="792" number="5">
    <region x="329.16" y="662.22" width="231.28" height="56.3" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="47.4" spacing="0.0">[15] A. Wright, "Get smart," Communications of the ACM, vol. 52, </line>
      <line x_offset="18.0" y_offset="38.52" spacing="-0.02">2009, pp. 15-16. </line>
      <line x_offset="0.0" y_offset="26.88" spacing="2.63">[16] J.W.P. Ng, B.P.L. Lo, O. Wells, M. Sloman, C. Toumazou, N. </line>
      <line x_offset="18.0" y_offset="17.88" spacing="0.1">Peters, A. Darzi, and G.Z. Yang, "Ubiquitous monitoring </line>
      <line x_offset="18.0" y_offset="8.88" spacing="0.1">environment for wearable and implantable sensors (UbiMon)," </line>
      <line x_offset="18.0" y_offset="0.0" spacing="-0.02">2004 </line>
    </region>
    <region x="68.16" y="630.18" width="231.39" height="88.46" line_height="8.9" font="OCAONP+TimesNewRoman">
      <line x_offset="0.0" y_offset="79.56" spacing="0.0">[11] A. Rubin, "The future of mobile," Offical Google Blog, 2008. </line>
      <line x_offset="0.0" y_offset="67.92" spacing="2.74">[12] S. Poduri and G. Sukhatme, "Constrained coverage for mobile </line>
      <line x_offset="18.0" y_offset="59.04" spacing="-0.02">sensor networks," IEEE; 1999, 2004, pp. 165-171. </line>
      <line x_offset="0.0" y_offset="47.4" spacing="2.74">[13] Kirby Chiang, Chi Cheng Chu, B. Prabhu, and R. Gadh, "In the </line>
      <line x_offset="18.0" y_offset="38.4" spacing="0.1">direction of a sensor mapping platform based on cellular phone </line>
      <line x_offset="18.0" y_offset="29.4" spacing="0.1">networks," Wireless Telecommunications Symposium, 2008. </line>
      <line x_offset="18.0" y_offset="20.52" spacing="-0.02">WTS 2008, 2008, pp. 348-353. </line>
      <line x_offset="0.0" y_offset="8.88" spacing="2.74">[14] Google Projects for Android," Google Projects for Android, </line>
      <line x_offset="18.0" y_offset="0.0" spacing="-0.02">2009. </line>
    </region>
    <region x="300.0" y="43.28" width="12.0" height="7.26" line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">
      <line x_offset="0.0" y_offset="0.0" spacing="0.0">121</line>
    </region>
  </page>
</pdf>
