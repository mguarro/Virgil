<?xml version="1.0"?>
<pdf>
  <page width="612" height="792" number="1">
    <region x="97.63" y="737.85" width="416.74" height="9.08"
line_height="9.08" font="XPTMOY+TimesNewRomanPSMT">The 6th IEEE
International Conference on Wireless, Mobile, and Ubiquitous Technologies
in Education</region>
    <region x="78.96" y="677.85" width="457.51" height="31.62"
line_height="15.54" font="OCAONN+TimesNewRoman,Bold">Development of
Web-based Japanese Mimicry and Onomatopoeia Learning Assistant System with
Sensor Network</region>
    <region x="147.0" y="581.89" width="320.72" height="60.23"
line_height="11.03" font="OCAONP+TimesNewRoman">Bin Hou, Hiroaki Ogata,
Masayuki Miyata, Mengmeng Li, Yoneo Yano Dept. of Information Science and
Intelligent Systems The University of Tokushima Tokushima, Japan
houbin2008@is.tokushima-u.ac.jp</region>
    <region x="54.0" y="444.69" width="245.61" height="113.53"
line_height="9.96" font="OCAOOB+TimesNewRoman,BoldItalic">Abstract- In this
paper, we propose a web-based Japanese mimicry and onomatopoeia learning
assistant system (JAMIOLAS). In our previous studies, we have proposed
context-aware language learning assistant systems that used wearable sensor
and sensor network respectively, and attended good results. In order to use
this learning model in broader area and more general scene, we are trying
to realize the system on website, and using on-line information as sensor
data from global sensor network. Besides, in order to support more words,
we are also using on-line multimedia such as video or picture to create the
context for learning.</region>
    <region x="262.42" y="424.41" width="36.69" height="9.96"
line_height="9.96" font="OCAOOB+TimesNewRoman,BoldItalic">language</region>
    <region x="54.0" y="414.09" width="203.74" height="20.28"
line_height="9.96" font="OCAOOB+TimesNewRoman,BoldItalic">Keywords-mimicry;
onomatopoeia; sensor; learning; context-aware learning; ubiquitous
learning</region>
    <region x="54.0" y="75.73" width="246.23" height="324.25"
line_height="11.03" font="OCAONP+TimesNewRoman">I. NTRODUCTIONI
Context-aware computing [1] will help in the organization and mediation of
social interactions wherever and whenever these contexts might occur [2].
Context-aware computing makes it possible to learning foreign language
words related to people's feeling more comfortably. Computer Supported
Ubiquitous Learning (CSUL) has integrated high mobility with embedded
computing environments [3,4]. We are focusing on applying CSUL to language
learning and are investigating computer supported ubiquitous learning [4].
We proposed context-aware language learning assistant system called
JAMIOLAS [5-7] for learning Japanese mimicry and onomatopoeia (MIO) words.
The previous two studies used wearable sensors and sensor network
respectively to detect the context automatically and achieved certain
effect. However, it still cannot meet learner needs. Therefore, in this
paper we propose an improved system named JAMIOLAS 3.0 that can support
learning MIO by using sensor data. IMICRY ND NOMATOPOEIA M A O II. JAPANESE
Mimicry words are imitating situations and body movements while
onomatopoeia shows sounds of something [7]. Japanese is very rich in it. It
is very important but very &#x2022; &#x2022; difficult to learn because of
following aspects: Explanation: Nearly all of MIO words are just feeling of
Japanese. Translation: Difficult to find the word that has the exactly same
meaning in other language.</region>
    <region x="315.0" y="73.21" width="246.31" height="486.09"
line_height="11.03" font="OCAONP+TimesNewRoman">Writing: Most of MIO words
are written in hiragana &#x2022; or katakana (Japanese syllabify), not in
kanji. It is easy to pronounce but difficult to understand. Hearing and
Saying: The pronunciation of MIO &#x2022; usually has twice repetitions. It
may cause the illusion of hearing and judge the different words as same
one. Meaning: MIO words have many synonyms and &#x2022; much assonance.
Situation: Some are only used in specific situation. &#x2022; For example,
"jime jime" means muggy, dump and humid, but it almost be used only in a
rainy season. Most of the MIO words are used to describe the speaker's
feeling. In order to know the speaker's feeling, we attempt to acquire
user's context with sensor. 1.0 A 2.0 ND III. JAMIOLAS JAMIOLAS 1.0 is
implemented by wearable sensors called Phidgets (physical widgets) [8] and
a Tablet PC (HP T1100) . When learning, the learner must wear Phidgets
connected to the system, and select a MIO as answer that is most suitable
for the situation in the question generated by system. However, when
learning, sometimes learners do not know where he/she could learn the MIO.
Learner must carry the system when using it, so it is not so convenient.
JAMIOLAS 2.0 use the wireless sensor network instead of wearable sensor,
and use RFID to recognize user's position. However, most of MIO words
cannot be supported by it, and it can only be used in limited area. For
these issues, we propose JAMIOLAS 3.0 to support learning MIO. 3.0 IV.
JAMIOLAS A. Context and sensor There are three important aspect of context:
Where you are, who you are with, and what resources are nearby [9]. Context
includes not only user's location, but also the lighting, noise level,
social situation and so on [10]. Human being usually gets the feeling from
environment by five senses including seeing, hearing, smelling, tasting and
touching. It is possible to get such context with sensors. The context can
be classified as two types - can be created by computer (scene, sound) and
cannot be created by computer (weather).</region>
    <region x="300.0" y="43.28" width="12.0" height="7.26"
line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">117</region>
    <region x="50.0" y="33.28" width="142.25" height="17.26"
line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">978-0-7695-3992-8/10
$26.00 &#xA9; 2010 IEEE DOI 10.1109/WMUTE.2010.24</region>
  </page>
  <page width="612" height="792" number="2">
    <region x="62.76" y="708.9" width="34.28" height="8.9"
line_height="8.9" font="OCAONP+TimesNewRoman">TABLE I.</region>
    <region x="97.2" y="700.96" width="192.54" height="16.96"
line_height="7.17" font="OCAONP+TimesNewRoman">, RELATIONSHIP BETWEEN BODY
SENSE CONTEXT AND SENSOR</region>
    <region x="56.28" y="667.74" width="60.24" height="23.66"
line_height="8.9" font="OCAONN+TimesNewRoman,Bold">Body Sense
Seeing</region>
    <region x="222.15" y="622.26" width="66.69" height="69.14"
line_height="8.9" font="OCAONP+TimesNewRoman">Sensor Light Sensor Image
Sensor Sound Sensor Temperature Sensor</region>
    <region x="139.15" y="622.26" width="54.46" height="69.14"
line_height="8.9" font="OCAONP+TimesNewRoman">Context Light Scene Sound
Temperature</region>
    <region x="56.28" y="622.26" width="27.84" height="24.26"
line_height="8.9" font="OCAONP+TimesNewRoman">Hearing Feeling</region>
    <region x="344.64" y="602.41" width="200.11" height="11.03"
line_height="11.03" font="OCAONP+TimesNewRoman">(A)Quiz for context (B)Give
a wrong answer</region>
    <region x="54.0" y="511.09" width="246.28" height="101.63"
line_height="11.03" font="OCAONP+TimesNewRoman">B. Implementation Figure 1
shows the architecture of system. The weather information and media files
are learning stuff in this system. We are using real-time on-line weather
service as sensor network. As the feeling is different one by one, the
system will use the voting mode to decide the proximate select to the
weather or media. We plan to use mobile as client, but for limitation on
condition, we have to implement this system on web side at
current.</region>
    <region x="339.12" y="473.82" width="196.63" height="24.17"
line_height="8.9" font="OCAONP+TimesNewRoman">(D)Word test (C)Give a right
answer Figure 2. Student's interface</region>
    <region x="125.76" y="361.38" width="101.47" height="8.9"
line_height="8.9" font="OCAONP+TimesNewRoman">Figure 1. System
architecture</region>
    <region x="54.0" y="130.93" width="246.12" height="214.19"
line_height="11.03" font="OCAONP+TimesNewRoman">C. System interface and
function 1) Student's interface This system mainly supports the following
functions for students. Learning by weather information: There are two
&#x2022; modes: Fix mode - system will show uses user's default location,
and generate a quiz to ask learner how to describe the current weather with
MIO; Tour mode - learner must set the coordinate first. When learner gave a
right answer, he/she can view the example and media for each selection or
enter the test mode to take a test. Learning by media: Learner choose a
media file and &#x2022; give select a MIO word that is most suitable for
this media. Finally there is a test function for learner Learning in free
mode: the free mode likes a media &#x2022; dictionary, user can look up a
word and the result is composed of examples and media. Figure 2 shows a
typical learning flow in this system.</region>
    <region x="315.0" y="122.29" width="246.21" height="340.67"
line_height="11.03" font="OCAONP+TimesNewRoman">2) Teacher's interface The
teacher can use all the functions available for students. In addition, the
teacher has access to another two functions. Evaluate weather/media: The
teacher can vote a &#x2022; word for weather/media. There are some
restrictions for evaluation. The user who has the role of teacher can only
vote for the weather once within one hour unless the weather information
has been updated. And each media file can only be voted for once by each
user. Word management: The user who has the role of &#x2022; teacher can
manage words. This function contains the common CRUD (create, retrieve,
update, delete), word example management, and weather word management.
VALUATION V. E A. Method We have done an experiment to compare JAMIOALS 3.0
to traditional method. After voting by Japanese students, we planned to
prepare 10 words as test data in this experiment (Table II). 6 Japanese
learners took the part of student. In them, 1 is living in Tokushima
(Japan), 3 are living in Tokyo (Japan) and 2 are living in Dalian (China).
They were divided into two groups: one used dictionary first, and another
used system. After 20 minutes, they exchanged, and continue to learn for 20
minutes. We put pre-test, mid-test and post-test in this experiment.
Finally, they answered a questionnaire that is 5 ranges from 1 to
5.</region>
    <region x="300.0" y="43.28" width="12.0" height="7.26"
line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">118</region>
  </page>
  <page width="612" height="792" number="3">
    <region x="142.56" y="708.9" width="119.76" height="8.9"
line_height="7.17" font="OCAONP+TimesNewRoman">USED IN THE EXPERIMENTATION
MIO</region>
    <region x="88.56" y="708.9" width="37.04" height="8.9"
line_height="8.9" font="OCAONP+TimesNewRoman">TABLE II.</region>
    <region x="491.88" y="645.43" width="48.14" height="19.76"
line_height="8.24" font="OCAONP+TimesNewRoman">Dictionary JAMIOLAS
3.0</region>
    <region x="57.48" y="637.62" width="238.69" height="62.06"
line_height="8.9" font="OCAONP+TimesNewRoman">Context MIO
Weather(Temperature) hinyari, nuku nuku Scene(Posture of walking) uro uro,
tyoko tyoko, noshi noshi, yochi yochi Sound(Sound of animals) ka ka, ga ga,
gero gero, tyun tyun</region>
    <region x="371.64" y="579.9" width="132.08" height="8.9"
line_height="8.9" font="OCAONP+TimesNewRoman">Figure 4. Total increased
score on type</region>
    <region x="54.0" y="514.57" width="246.07" height="102.71"
line_height="11.03" font="OCAONP+TimesNewRoman">B. Result We did three
tests for each learner during the experiments and administered a
questionnaire. These supplied us with our experimental data. Although this
is an initial experiment, with the analysis of the experimental data we can
see the following results. 1) The overall results The following chart
(Figure 3) shows the results of the pre-test, mid-test and
post-test.</region>
    <region x="237.12" y="425.12" width="40.49" height="30.5"
line_height="6.98" font="OCAONP+TimesNewRoman">Dictionary First Group
System First Group</region>
    <region x="179.78" y="396.65" width="24.25" height="7.73"
line_height="7.73" font="OCAONP+TimesNewRoman">Post-test</region>
    <region x="139.57" y="396.65" width="23.92" height="7.73"
line_height="7.73" font="OCAONP+TimesNewRoman">Mid-test</region>
    <region x="100.32" y="396.65" width="21.52" height="7.73"
line_height="7.73" font="OCAONP+TimesNewRoman">Pre-test</region>
    <region x="89.4" y="373.14" width="174.54" height="8.9"
line_height="8.9" font="OCAONP+TimesNewRoman">Figure 3. Average increased
score in the experiment</region>
    <region x="315.0" y="284.77" width="246.2" height="285.23"
line_height="11.03" font="OCAONP+TimesNewRoman">Figure 4 shows the increase
in score for each type of context. All three types were learned better by
using the system than by using the dictionary. It shows that words related
to sound (onomatopoeia) showed the highest increase in score both by the
dictionary and the system. For the weather context, only the system
provided benefit; in this experiment, the dictionary had no effect on
learning the words related to weather. On the contrary, one learner changed
their original right answer into a wrong answer after learning the weather
words using the dictionary and got a lower score. This suggests that the
dictionary cannot effectively explain MIO related to weather. 3)
Questionnaire Table III shows the results of the questionnaire. According
to Question (1), we learned that the system is helpful for learning mimicry
and onomatopoeia. The highest average score and lowest standard deviation
of Question (1) in the questionnaire shows the system is generally
considered useful. Question (2) asked whether the answers of the presented
quiz were appropriate to the situation or not. In this case the result was
less satisfactory, it demonstrates the biggest problem of learning MIO is
with feelings, because by the standard deviation we can see the feeling is
different depending upon the person. Therefore, in the future we should
seek for a method to make the words more appropriate to the
situation.</region>
    <region x="418.68" y="264.42" width="89.4" height="8.9"
line_height="7.17" font="OCAONP+TimesNewRoman">RESULT OF
QUESTIONNAIRE</region>
    <region x="364.68" y="264.42" width="39.68" height="8.9"
line_height="8.9" font="OCAONP+TimesNewRoman">TABLE III.</region>
    <region x="315.0" y="121.38" width="243.4" height="133.82"
line_height="8.9" font="OCAONP+TimesNewRoman">Question Avg SD 1 Were you
able to learn mimicry and onomatopoeia by this 4.8 0.4 system? 2 Was the
answer of the presented quiz appropriate to the 3.8 0.8 situation? 3 Were
you able to learn mimicry and onomatopoeia with 4.2 0.8 enjoyment? 4 Is the
system easy to use? 4.2 0.8 5 Do you want to learn by using this system in
the future? 4.3 0.5 6 Which do you think enhances learning more, this
system or 4.5 0.5 traditional learning? (this system: 5, traditional
learning: 1)</region>
    <region x="54.0" y="110.77" width="246.14" height="252.35"
line_height="11.03" font="OCAONP+TimesNewRoman">Figure 3 shows the average
score increases in the midtest and post-test. As shown in this figure, the
system first group showed better increased score than the dictionary first
group. After the mid-test and the group changeover, the final results from
the system first group were higher than for the dictionary first group.
Although the data from this small sample size would not bear statistical
analysis, the results suggest that JAMIOLAS 3.0 can be more effective than
a dictionary for learning MIO. From feedback, we learned that when users in
the system first group used JAMIOLAS 3.0 from the beginning, both the
system and the words were new to them, and they made full use of system.
However, when users had first learned the meaning of words from the
dictionary, they lost the freshness of the words, so they felt that they
had already understood the meaning and the system was underutilized. In the
questionnaire, most learners suggested we add the meanings in the system,
and the system should provide an explanation when the results are shown. 2)
Results on type of context There were three types of context in the
evaluation experiment. We used the mid-test data to plot this in order to
see the effects of the systems for different types of context.</region>
    <region x="315.0" y="74.65" width="245.98" height="32.87"
line_height="11.03" font="OCAONP+TimesNewRoman">Results to Question (3)
showed learners were able to effectively learn using this system, which can
motivate them to learn language. For Question (4), most learners
thought</region>
    <region x="300.0" y="43.28" width="12.0" height="7.26"
line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">119</region>
  </page>
  <page width="612" height="792" number="4">
    <region x="315.0" y="412.8" width="248.54" height="307.89"
line_height="11.03" font="OCAONP+TimesNewRoman">Using embedded mobile
sensors to acquire sensor &#x2022; data anywhere at any time. The current
system is not using sensors that we have deployed, but the sensor data is
received from the global sensor network that is supplied by an on-line
weather provider. In the future, sensors will be embedded in each mobile,
as users' phones will know a lot about the world around them [11,12]. Our
future plan is to support Japanese MIO learning with the sensors that have
been embedded in the next generation of mobile technology, especially
mobiles using the Android operating system [13]. Android has already
supplied an API for 8 types of sensor including accelerometer, gyroscope,
light, magnetic field, orientation, pressure, proximity and temperature
sensors [14,15]. Introducing biosensors to support learning MIO &#x2022;
related to emotion. Because most MIO are just used to describe one's
feelings, there are many words related to emotion, but these cannot be
supported by the current system. One way to recognize the emotion is using
biosensors to acquire information from the human body [16]. In addition, we
must look for an approach to making the questions in the system more
appropriate to the learning context. We should also consider how to
introduce the meaning of words into the system more effectively. However,
we will improve the system and conduct further evaluations to gather more
experimental data.</region>
    <region x="329.16" y="82.5" width="231.73" height="319.22"
line_height="8.9" font="OCAONP+TimesNewRoman">[1] G.D. Abowd and E.D.
Mynatt, "Charting past, present, and future research in ubiquitous
computing," ACM Transactions on Computer-Human Interaction (TOCHI), vol. 7,
2000, pp. 29-58. [2] G. Fischer, "User Modeling in Human-Computer
Interaction," User Modeling and User-Adapted Interaction, vol. 11, 2001,
pp. 65-86. [3] Y.S. Chen, T.C. Kao, J.P. Sheu, and C.Y. Chiang, "A mobile
scaffolding-aid-based bird-watching learning system," 2002, pp. 15-22. [4]
H. Ogata and Y. Yano, "Context-aware support for computersupported
ubiquitous learning," 2004, pp. 27-34. [5] M. Miyata, H. Ogata, T. Kondo,
and Y. Yano, "JAMIOLAS 2.0: Supporting to Learn Japanese Mimetic Words and
Onomatopoeia with Wireless Sensor Networks," Taipei: 2008, pp. 643-650. [6]
H. Ogata, T. Kondo, C. Yina, Y. Liub, and Y. Yanoa, "Computer Supported
Ubiquitous Learning Environment for Japanese Mimicry and Onomatopoeia with
Sensors," Supporting Learning Flow Through Integrative Technologies, 2007,
p. 463. [7] H. Ogata, C. Yin, and Y. Yano, "JAMIOLAS: Supporting Japanese
Mimicry and Onomatopoeia Learning with Sensors," 2006, pp. 111-115. [8] S.
Greenberg and C. Fitchett, "Phidgets: easy development of physical
interfaces through physical widgets," ACM Press, 2001, pp. 209-218. [9] J.
Lee, S. Oh, and M. Jeon, "A New Context-Aware Learning System for
Predicting Services to Users in Ubiquitous Environment," International
Symposium on Ubiquitous VR, 2007, p. 1. [10] B. Schilit, N. Adams, and R.
Want, "Context-aware computing applications," Mobile Computing Systems and
Applications, 1994. WMCSA'08. First Workshop on on Mobile Computing Systems
and Applications, Santa Cruz, CA, USA: 1994, pp. 8590.</region>
    <region x="54.0" y="73.93" width="246.26" height="646.31"
line_height="11.03" font="OCAONP+TimesNewRoman">this system was easy to
use. Question (3) and Question (4) also have high standard deviations, and
these tell us that we must improve the interface of system and provide
better learning experience. Question (5) shows that users like using this
kind of system and would use it again. Question (6) shows that compared to
traditional learning; most learners thought this system was more effective
for learning MIO. Apart from the above questions, we also asked for
feedback regarding the difficulties users encountered as they were learning
MIO. All of them said the words were difficult to remember, but two of the
users could not explain why. Others said there are too many MIO words, and
they could not guess the meaning using the limited information. In
addition, although users had learned the meaning, because they had no
intuitive feeling and no environment, it was easy to forget MIO words.
These answers will help us to improve the system in future work. We also
asked the learners to make some general comments on the system. One user
said this system was very interesting, they could easily learn the meaning
the words in an environment, and it is a really good way to explain the MIO
words by feeling. Most of users said when the answer of a quiz is given,
they could only see what was right or wrong, but could not learn why. If
the meaning could be shown at that time, it will be more conducive to
remembering the words. One user said that in the test there were sometimes
too many questions for one word. This is in fact a result of the current
algorithm, which we can improve in the future. Another user also said the
video was not so clear; however this problem cannot be solved at present
because of the limitation of technology. During the evaluation, we learned
that the system can explain the words that related to feeling very well by
sensor data, and that learning efficacy and quality are better than
traditional learning. VI. C ONCLUSION AND FUTURE WORK In this paper, we
described an improved context-aware learning system named JAMIOLAS 3.0 that
can support the learning of Japanese MIO words in a sensory learning mode
with global sensor network. This is an on-line system that can be used in a
broader area than previous versions. It uses on-line weather information as
global weather sensor data. In addition, it also uses media (video/audio)
as image/sound sensor data and can support learning more words than before.
Two methods are used in this system: created context and detecting context
automatically to provide the right MIO for the learner's context. Through
an initial evaluation experiment, we can see that this system is effective
for learning MIO. The feeling related to a weather situation can be
explained by the system more effectively than by dictionary. However, on
the media side, although the media can be explained by the system better
than by the dictionary, the results and feedback suggest that the
dictionary is still useful. With the help of the dictionary, a learner can
learn the words using both meaning and feeling. In the future, we will
focus on making the questions more appropriate to environment and weather
data, and we will also consider how to use multiple conditions of weather
information. Our future research will be in the following
directions:</region>
    <region x="300.0" y="43.28" width="12.0" height="7.26"
line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">120</region>
  </page>
  <page width="612" height="792" number="5">
    <region x="329.16" y="662.22" width="231.28" height="56.3"
line_height="8.9" font="OCAONP+TimesNewRoman">[15] A. Wright, "Get smart,"
Communications of the ACM, vol. 52, 2009, pp. 15-16. [16] J.W.P. Ng, B.P.L.
Lo, O. Wells, M. Sloman, C. Toumazou, N. Peters, A. Darzi, and G.Z. Yang,
"Ubiquitous monitoring environment for wearable and implantable sensors
(UbiMon)," 2004</region>
    <region x="68.16" y="630.18" width="231.39" height="88.46"
line_height="8.9" font="OCAONP+TimesNewRoman">[11] A. Rubin, "The future of
mobile," Offical Google Blog, 2008. [12] S. Poduri and G. Sukhatme,
"Constrained coverage for mobile sensor networks," IEEE; 1999, 2004, pp.
165-171. [13] Kirby Chiang, Chi Cheng Chu, B. Prabhu, and R. Gadh, "In the
direction of a sensor mapping platform based on cellular phone networks,"
Wireless Telecommunications Symposium, 2008. WTS 2008, 2008, pp. 348-353.
[14] Google Projects for Android," Google Projects for Android,
2009.</region>
    <region x="300.0" y="43.28" width="12.0" height="7.26"
line_height="7.26" font="KYOAFV+TimesNewRomanPSMT">121</region>
  </page>
</pdf>
